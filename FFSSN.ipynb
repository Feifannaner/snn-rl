{
 "metadata": {
  "name": "",
  "signature": "sha256:33dcbb2675de1f6d3ca7f839422dcac97719fd3d1103aad91c60cea24611299c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.3em;text-decoration:underline;font-weight:bold'>Reinforcement Learning Spiking Neural Network Methods</div><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Results</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import YouTubeVideo; YouTubeVideo(\"DUZldskw51A\", autoplay=1, width=1024, height=576,loop=1,playlist=\"DUZldskw51A\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"1024\"\n",
        "            height=576\"\n",
        "            src=\"https://www.youtube.com/embed/DUZldskw51A?playlist=DUZldskw51A&loop=1&autoplay=1\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<IPython.lib.display.YouTubeVideo at 0x346fef0>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>The above video shows how after training the model the neurons are specialized to spike when the input letters of A,B,C, and D were\n",
      "presented as stimulus.  Performance is commonly 87.5% accuracy to input.  Letters were presented sequentially for 100ms at a time 3 times in a row.  As can be seen spikes occur when a neuron's voltage reaches 10mv.  Once a spike occurs the voltage of the other neurons is inhibited which is visible as their large drops on the plot.  The inihibtion and excitation is greater than wanted (-65mv at some points) but work is being done to scale that better.  Also 'B' and 'D' have the same neuron but that is due to them being visually so similar!  Overall the neurons specialized reasonably well to the input but there is room for greater performance as well.  <a href='/notebooks/furtherFormulas/3dAnimScatterPlotHdf5.ipynb'>Animation</a> <a href='/github/tartavull/snn-rl/blob/master/testVoltage.mp4'>Video Download</a><br><br>\n",
      "\n",
      "You may ask, how did the neurons get trained?  Let me explain!<br><br>\n",
      "\n",
      "Some introductory material to this work is availible at <a target=\"_blank\" href='https://github.com/tartavull/snn-rl/blob/master/README.md'>ReadMe</a> and <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/introduction.ipynb'>Intro</a>.  It is based on this <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a>.  Code is <a target=\"_blank\" href='https://github.com/tartavull/snn-rl'>here</a>.</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Training</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>Values are trained for the model by displaying each charactor after each other for one spike interval (100ms).</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Time and Refractory Period</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>Time is initally incremented and refractory period status variables are processed.  <a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/timeAndRefracCalcs.ipynb#timePeriodAndRefractoryCalcs'>Time&Refrac</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>First layer and Dirac function</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>Spikes from the first layer are represented as presynaptic spike times across a 100 epoch (100000ms) timeframe.  They are coded into a variable that is has lookups done in it to refer back to when spikes have occured.  Spikes found trigger a dirac function which is one of the cofactors in the dendrite and somaDirect equations which build their signal that leads to the soma potential.  <a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>Dirac Function</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Weight</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>The weights are set at random initially but at a range of values that allows dirac activated dendrite and someDirect signals to \n",
      "combine to create postsynaptic spikes (from output neurons) from the beginning or soon enough afterward.  After initial spiking has occured the reinforcement learning comes into effect where charactor input that causes a presynaptic spike and post synapic spike after that causes a weight increase.  The increase is defined by the weighting equations.  <a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>weight change</a>, \n",
      "<a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>returnDeltaW</a>, \n",
      "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#returnNewW'>returnNewW</a>\n",
      "<br><br><center>Weights of the Neurons After Training:<img src=\"http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/weights.jpeg\"><a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartGenerator.ipynb#returnNewW'>3dBarChartGenerator</a></center></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Tau</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>The learning rate for the dendrite, tau, is dependednt on the results of the weight calculation.  It creates faster learning when a stronger weight is present.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#tauDCalc'>Tau</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Resistance</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>Resistance is dependent on tau and assists with allowing the voltage to reach a spiking level even with lower weights and tau.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#resistanceCalc'>Resistance</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Dendrite Input</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em'>This is one receptor of presynaptic input that translates signals into other values that are passed to the soma.  \n",
      "The equation below specifies how.  Cofactors in the equation are devided by their units to normalize the values.  <br>Dendrite equ in Brian:  dv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt  \n",
      "<a target=\"_blank\" href='#mainCode'>Dendrite Equations</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Direct Input</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>Another receptor of presynaptic input.  <br>SomaDirect equ in Brian:  dv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt  \n",
      "<a target=\"_blank\" href='#mainCode'>SomaDirect Equations</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Lateral Inhibition</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>Due to competition amongst neurons inhibition signals are sent from one to another when they receive input.  A winner-take-all type of implementation was created where inhibition is auto tuned, the neuron with the greatest soma membrane potential change is the only one allowed to create a post-synaptic spike.  The membrane potential is scaled based on the inhibition.\n",
      "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/lateralInhibition.ipynb#lateralInhibition'>Lateral Inhibition</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Membrane Potential Charge (Um)</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>Um prior to lat. inh.: dprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
      "<br>Um after lat. inh.: dv/dt = v/(1*second): volt\n",
      "<a target=\"_blank\" href='#mainCode'>Soma Membrane Potential Equations</a><br><br><center>Spikes of Neurons During Training<img src=\"/github/tartavull/snn-rl/blob/master/img/trainingSpikes.jpg\">The dots represent a spike fired for an input charactor stimulus.  Notice how over time the neurons specialize in the way they designate themselves for only one charactor.  That is the reinforcement learning causing the neurons to specilize and in this example it takes toward 10000ms for that.  In some training simulations they do not all correctly become designated to one charactor.</center>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Check for resets</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>After each spike interval has passed some logic is included to reset values upon spike occurences.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#mainCode'>Resets</a></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Testing</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:1.5em;'>The weights and subsequently tau and resistance generated are used as the trained model values and tests are run to evaluate the performance.  Input charactors are presented three times in a row for three spike intervals (300ms total).  Observed spikes fired and not are compared to expected values and performance is reported.\n",
      "<a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#intitializeTrainedModelParameters'>intitializeTrainedModelParameters</a>, <a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#evaluateClassifierPerf'>evaluateClassifierPerf</a>, <a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#OutputEvaluationResults'>OutputEvaluationResults</a><br><br><center>Spikes Generated During Testing<img src=\"/github/tartavull/snn-rl/blob/master/img/testSpikes.png\">Further description of the simulation results is <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/analysisFurtherFormulas.ipynb'>here</a></center></div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Main code for the simulation:<a id='mainCode'></a></div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from architecture_further_formulas import *\n",
      "from furtherFormulas.cofactorCalculations import *\n",
      "from furtherFormulas.timeAndRefracCalcs import *\n",
      "from furtherFormulas.outputPrinting import *\n",
      "from furtherFormulas.testingProcesses import *\n",
      "from furtherFormulas.lateralInhibition import *\n",
      "\n",
      "timeAndRefrac = timeAndRefrac\t\n",
      "\n",
      "class gupta_paper:\n",
      "\n",
      "\tneuralnet = Network()\n",
      "\tdictionary = dictionary()\n",
      "\tspiketimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, spikesPerChar, epochs)\n",
      "\ttrainingSpikeTimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, 1, 300)\n",
      "\tLIK = SpikeGeneratorGroup(N=15, indices=spiketimes[:,0], times=spiketimes[:,1]*ms)\n",
      "\t# tauD and R lines below are to avoid an odd 'reference before assignment' error\n",
      "\ttauD = tauD\n",
      "\tW = W\n",
      "\tR = R\n",
      "\ttauM = tauM\n",
      "\tneuronIndex = neuronIndex\n",
      "\tgeneralClockDt = generalClockDt\n",
      "\trunTime = runTime\n",
      "\trunTimeScaling = runTimeScaling\n",
      "\tevaluateClassifier = evaluateClassifier\n",
      "\taccelerateTraining = accelerateTraining\n",
      "\tdiracScaling = diracScaling\n",
      "\tsomaDirectScaling = somaDirectScaling\n",
      "\tnegativeWeightReinforcement = negativeWeightReinforcement\n",
      "\tpositiveWeightReinforcement = positiveWeightReinforcement\n",
      "\ttimeAndRefrac = timeAndRefrac\t\n",
      "\ttestRun = testRun\n",
      "\tlatInhibSettings = latInhibSettings\n",
      "\n",
      "\tprint 'initial Weights\\n',W\n",
      "\n",
      "\tdef run_model(self):\n",
      "\t\tneuralnet = self.neuralnet\n",
      "\t\tdictionary = self.dictionary\n",
      "\t\teqs = Equations('''\n",
      "\t\t\tdv/dt = v/(1*second): volt\n",
      "\t\t\tdprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
      "\t\t\tRm = 80*mV : volt\n",
      "\t\t\ttauM = 30*ms : second\n",
      "\t        V : volt\n",
      "\t        DendI : volt\n",
      "\t        SynI : volt\n",
      "\t        v2 : volt\t\n",
      "\t\t\tUmSpikeFired : volt\t\n",
      "\t\t\tbeginRefrac : volt\n",
      "\t\t    ''')\t\t\t\n",
      "\n",
      "\t\tdendriteEqs = Equations('''\n",
      "\t\t\tdv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt\n",
      "\t\t\tV : volt\n",
      "\t        r : volt\n",
      "\t        w : volt\n",
      "\t        dirac : volt\n",
      "\t        tau : second\n",
      "\t        v2: volt\n",
      "\t\t\t''')\n",
      "\n",
      "\t\tdirectToSomaEqs = Equations('''\n",
      "\t\t\tdv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt\n",
      "\t\t\ttauS = 2*ms : second\n",
      "\t\t\tV : volt\n",
      "\t\t\tsummedWandDirac : volt\n",
      "\t\t\tv2 : volt\n",
      "\t\t\tspikeFired : boolean\n",
      "\t\t\t''')\t\t\n",
      "\n",
      "\t\tclass ADDSNeuronModel(NeuronGroup, gupta_paper): \n",
      "\t\t\tneuronIndex = self.neuronIndex\n",
      "\t\t\tgeneralClockDt = self.generalClockDt\n",
      "\t\t\tdef __init__(self, params):\n",
      "\t\t\t\tNeuronGroup.__init__(self, N=4, model=eqs,threshold='v>10*mV', reset='v=-0.002 * mV; dv=0; v2=10*mV;UmSpikeFired=1*mV;beginRefrac=1*mV;inhibitionVoltage=prelimV',refractory=8*ms,clock=Clock(dt=self.generalClockDt))\n",
      "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
      "\t\t\t\tdef additionToNetwork(): \n",
      "\t\t\t\t\tneuronIndex = self.neuronIndex\n",
      "\t\t\t\t\ttimeAndRefrac.spikeIntervalCounter = (floor(timeAndRefrac.time/timeAndRefrac.spikeIntervalUnformatted) * timeAndRefrac.spikeIntervalUnformatted)*10\n",
      "\n",
      "\t\t\t\t\tdef dendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive):\n",
      "\t\t\t\t\t\t# Below sequentially Dirac, Tau, then Resistance are calculated every end of a spike-time interval.\n",
      "\t\t\t\t\t\t# The resulting Dend I is added to the Um calc for the ADDS soma.\n",
      "\t\t\t\t\t\ttimeAndRefrac = self.timeAndRefrac\n",
      "\n",
      "\t\t\t\t\t\t# Dirac\n",
      "\t\t\t\t\t\tdendObj[neuronIndex].dirac = diracCalc(dendObj, neuronIndex, spiketimes, timeAndRefrac.time, timeAndRefrac.lastSpikeInterval)\n",
      "\n",
      "\t\t\t\t\t\t# Initialize weights\n",
      "\t\t\t\t\t\tif timeAndRefrac.time == 0.000:\n",
      "\t\t\t\t\t\t\tdend[neuronIndex].w = W[neuronIndex]*volt\n",
      "\n",
      "\t\t\t\t\t\tif (evaluationActive==False and timeAndRefrac.refractoryPointSwitch==True):\n",
      "\t\t\t\t\t\t\t# Only change weights of neuron fired\n",
      "\t\t\t\t\t\t\tif ADDSObj.UmSpikeFired[neuronIndex] == 1*mV:\n",
      "\t\t\t\t\t\t\t\t# Weights\n",
      "\t\t\t\t\t\t\t\tWeightChangeCalculation(neuronIndex, spiketimes, timeAndRefrac.time, self.negativeWeightReinforcement, self.positiveWeightReinforcement, M, dendObj)\t\n",
      "\t\t\t\t\t\t\t# Tau\n",
      "\t\t\t\t\t\t\ttauDCalc(neuronIndex, dendObj, self.W)\n",
      "\t\t\t\t\t\t\t# Resistance\n",
      "\t\t\t\t\t\t\tresistanceCalc(neuronIndex, dendObj, self.R, self.tauM)\n",
      "\n",
      "\t\t\t\t\t\t# TODO: check do I need additional loop below?\n",
      "\t\t\t\t\t\tfor indexOfDend in range(dictionaryLongitude):\n",
      "\t\t\t\t\t\t\tADDSObj.DendI[indexOfDend] = sum(dendObj[indexOfDend].v[:])*dendCalcScaling\n",
      "\n",
      "\t\t\t\t\tdef somaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj):\n",
      "\t\t\t\t\t\tdotProductWandDirac =  sum(w*d for w,d in zip(dendObj[neuronIndex].w[:], dendObj[neuronIndex].dirac[:]))\n",
      "\t\t\t\t\t\tsomaDirectObj.summedWandDirac[neuronIndex] = ((dotProductWandDirac*volt)/(volt**2))*self.somaDirectScaling\n",
      "\n",
      "\t\t\t\t\t\tfor neuronNumber in range(dictionaryLongitude):\n",
      "\t\t\t\t\t\t\tADDSObj.SynI[neuronNumber] = somaDirectObj.v[neuronNumber]\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\tdef mainSimulationCalcs(ADDSObj, dendObj, somaDirectObj, spiketimes, evaluationActive):\n",
      "\t\t\t\t\t\t# dend then somaDirect calcs are done which are then used to set lat inhib.\n",
      "\t\t\t\t\t\t# Soma Um calcs are done automatically using equations entered for brian\n",
      "\t\t\t\t\t\t# once dend and somaDirect are updated\n",
      "\t\t\t\t\t\tpreTNorm = self.timeAndRefrac.time\n",
      "\t\t\t\t\t\ttNorm = preTNorm - (floor((preTNorm/.001)*.01) * .1)\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\tself.timeAndRefrac = timePeriodAndRefractoryCalcs(self.timeAndRefrac)\n",
      "\n",
      "\t\t\t\t\t\tif (evaluationActive==True):\n",
      "\t\t\t\t\t\t\tintitializeTrainedModelParameters(dendObj)\n",
      "\n",
      "\t\t\t\t\t\t# Option to accelerate computations for training\n",
      "\t\t\t\t\t\tif self.accelerateTraining == False or (evaluationActive == False and tNorm <= .005 or tNorm >= .096):\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tif self.accelerateTraining == True and (tNorm >= .096 and tNorm < .099):\n",
      "\t\t\t\t\t\t\t\tfor i in range(dictionaryLongitude):\n",
      "\t\t\t\t\t\t\t\t\tADDSObj.DendI[i]=0*mV\n",
      "\t\t\t\t\t\t\t\t\tADDSObj.SynI[i]=0*mV\n",
      "\t\t\t\t\t\t\t\t\tADDSObj.prelimV[i]=0*mV\n",
      "\t\t\t\t\t\t\t\t\tADDSObj.v[i]=0*mV\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\tfor i2 in range(len(dend[0].v)):\n",
      "\t\t\t\t\t\t\t\t\t\tdendObj[i].v[i2] = 0*mV\n",
      "\t\t\t\t\t\t\t\t\tsomaDirectObj.v[i] = 0*mV\t\t\n",
      "\n",
      "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude):\n",
      "\t\t\t\t\t\t\t\tdendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive)\n",
      "\n",
      "\t\t\t\t\t\t\t\tsomaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj)\n",
      "\n",
      "\t\t\t\t\t\t\tADDSObj.v, self.latInhibSettings = lateralInhibition(ADDSObj, self.timeAndRefrac, self.latInhibSettings)\n",
      "\n",
      "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude): \n",
      "\t\t\t\t\t\t\t\tADDSObj.v2, somaDirectObj.v2, self.timeAndRefrac = checkForResets(neuronIndex, ADDSObj, dendObj, somaDirectObj, self.timeAndRefrac)\n",
      "\n",
      "\t\t\t\t\t\t\tADDSObj.UmSpikeFired, self.testRun = evaluateClassifierPerf(ADDSObj, self.testRun)\n",
      "\t\t\t\t\tif self.evaluateClassifier == False:\n",
      "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.trainingSpikeTimes, False)\n",
      "\t\t\t\t\telse:\n",
      "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.spiketimes, True)\t\t\t\t\t\n",
      "\n",
      "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\t\t\n",
      "\n",
      "\t\tclass DendriteNeuronModel(NeuronGroup):\n",
      "\t\t\tgeneralClockDt = self.generalClockDt\n",
      "\t\t\tdef __init__(self, params): \n",
      "\t\t\t\tNeuronGroup.__init__(self, N=15, model=dendriteEqs,clock=Clock(dt=self.generalClockDt))\n",
      "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
      "\t\t\t\tdef additionToNetwork(): \n",
      "\t\t\t\t\tplaceHolderForLaterContent = True\n",
      "\t\t\t\tself.contained_objects.append(additionToNetwork)\n",
      "\n",
      "\t\tclass SomaDirectNeuronModel(NeuronGroup): \n",
      "\t\t\tgeneralClockDt = self.generalClockDt\n",
      "\t\t\tdef __init__(self, params): \n",
      "\t\t\t\tNeuronGroup.__init__(self, N=4, model=directToSomaEqs,clock=Clock(dt=self.generalClockDt))\n",
      "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
      "\t\t\t\tdef additionToNetwork(): \n",
      "\t\t\t\t\tplaceHolderForLaterContent = True\n",
      "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\n",
      "\n",
      "\t\tdend = [None]*dictionaryLongitude\n",
      "\t\tfor firstLayerIndex in range(dictionaryLongitude):\n",
      "\t\t\tdend[firstLayerIndex] = DendriteNeuronModel(15)\t\n",
      "\t\tsomaDirect = SomaDirectNeuronModel(4)\n",
      "\t\tADDS = ADDSNeuronModel(self)\t\t\t\n",
      "\t\tM = SpikeMonitor(ADDS)\n",
      "\t\tMv = StateMonitor(ADDS, 'V', record=True)\n",
      "\t\tUmM = StateMonitor(ADDS, 'v2', record=True)\n",
      "\t\tself.M = M # for ipython compatibility\n",
      "\t\tself.UmM = UmM # for ipython compatibility\n",
      "\t\tfor firstLayerIndex in range(dictionaryLongitude):\n",
      "\t\t\tneuralnet.add(dend[firstLayerIndex])\n",
      "\n",
      "\t\tneuralnet.add(ADDS)\n",
      "\t\tneuralnet.add(M)\n",
      "\t\tneuralnet.add(UmM)\t\t\n",
      "\t\tself.runTime *= self.runTimeScaling # scaling factor\n",
      "\t\tneuralnet.run(self.runTime,report='text')\n",
      "\n",
      "\t\tOutputEvaluationResults(dend, self.testRun)\n",
      "\n",
      "\t\tneuronToPlot = 1\n",
      "\t\tcolors = ['r']*1+['g']*1+['b']*1+['y']*1\n",
      "\t\tcolors = ['blue', 'green', 'magenta', 'cyan']\n",
      "\t\tsubplot(211)\n",
      "\t\tplot(M.t/ms, M.i, '.')\n",
      "\t\tlegend(['A','B','C','D'], loc='upper left')\t\t\t\n",
      "\t\tsubplot(212)\n",
      "\t\tplot(UmM.t, UmM.v2.T/mV)\t\n",
      "\t\txlabel('Time (ms)')\n",
      "\t\tylabel('Membrane Potential (mV)')\n",
      "\t\tshow()\t\n",
      "\n",
      "\tdef __init__(self):\n",
      "\t\tself.run_model()\n",
      "\n",
      "def main():\n",
      "\trun_gupta_paper = gupta_paper()\n",
      "\n",
      "if  __name__ =='__main__':main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}